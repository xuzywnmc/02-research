Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Ma2019,
abstract = {Image fusion can be regarded as how to integrate complementary information in images with different focal region together. However, except for complementary information, there is also redundant information in source images. To obtain a better fused image, an efficient image fusion method based on sparse representation and optimal solution was proposed in this paper. Firstly, we obtained adaptive dictionaries based on source images themselves by K-means singular value decomposition. Then, we combined a fixed dictionary with adaptive dictionaries to obtain the joint dictionary. By sparse coding source images with the final joint dictionary, complementary and redundant components could be apart. Next, the optimum theory was employed to fuse complementary components and an optimal solution could be obtained by orthogonal matching pursuit. A fused image was constructed by sparse representation at last. Experimental results showed the proposed method had better visual effects and objective valuable index values.},
author = {Ma, Xiaole and Hu, Shaohai and Liu, Shuaiqi and Fang, Jing and Xu, Shuwen},
doi = {10.1016/j.image.2019.06.002},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Multi-focus image fusion based on joint sparse representation and optimumtheory.pdf:pdf},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {Joint sparse representation,Multi-focus image fusion,Optimum theory,Orthogonal matching pursuit},
number = {May},
pages = {125--134},
publisher = {Elsevier Ltd},
title = {{Multi-focus image fusion based on joint sparse representation and optimum theory}},
url = {https://doi.org/10.1016/j.image.2019.06.002},
volume = {78},
year = {2019}
}
@article{Du2017,
abstract = {A novel method for performing anatomical magnetic resonance imaging-functional (positron emission tomography or single photon emission computed tomography) image fusion is presented. The method merges specific feature information from input image signals of a single or multiple medical imaging modalities into a single fused image, while preserving more information and generating less distortion. The proposed method uses a local Laplacian filtering-based technique realized through a novel multi-scale system architecture. First, the input images are generated in a multi-scale image representation and are processed using local Laplacian filtering. Second, at each scale, the decomposed images are combined to produce fused approximate images using a local energy maximum scheme and produce the fused residual images using an information of interest-based scheme. Finally, a fused image is obtained using a reconstruction process that is analogous to that of conventional Laplacian pyramid transform. Experimental results computed using individual multi-scale analysis-based decomposition schemes or fusion rules clearly demonstrate the superiority of the proposed method through subjective observation as well as objective metrics. Furthermore, the proposed method can obtain better performance, compared with the state-of-the-art fusion methods.},
author = {Du, Jiao and Li, Weisheng and Xiao, Bin},
doi = {10.1109/TIP.2017.2745202},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/LLF{\_}IOI.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Image fusion,interest-based rule,multi-scale decomposition},
number = {12},
pages = {5855--5866},
publisher = {IEEE},
title = {{Anatomical-Functional Image Fusion by Information of Interest in Local Laplacian Filtering Domain}},
volume = {26},
year = {2017}
}
@article{Hu2019,
abstract = {In this paper, a Spare Representation (SR) based fusion quality evaluation and analysis method is proposed. This method employs Joint Sparse Representation (JSR) to extract the source image remnants after fusion. These atom-level remnants indicate the fusion quality intuitively, and permit the analysis of fusion effect in learned feature space. Our analysis results indicate that high salient atoms always present poor expressions in fusion results. An improved fusion rule is designed to emphasis high salient atoms accordingly. In experiments, the effectiveness of our method was verified and the characteristics of atom JSR remnants were investigated in detail first. Then the new fusion rule was tested to demonstrate the value of JSR remnant analysis. The objective and subjective comparison results indicate that the proposed analytical evaluation metric can measure fusion quality and analysis atom fusion effect accurately. The new fusion rule provides a valuable alternative for SR fusion algorithm design.},
author = {Hu, Yanxiang and Gao, Qian and Zhang, Bo and Zhang, Juntong},
doi = {10.1016/j.jvcir.2019.04.005},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/On the use of joint sparse representation for image fusion quality evaluation and analysis.pdf:pdf},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Atom remnant analysis,Image fusion,Joint sparse representation,Quality evaluation,Sparse representation},
pages = {225--235},
publisher = {Elsevier Inc.},
title = {{On the use of joint sparse representation for image fusion quality evaluation and analysis}},
url = {https://doi.org/10.1016/j.jvcir.2019.04.005},
volume = {61},
year = {2019}
}
@article{Asha2019,
abstract = {Recently, medical image fusion has emerged as an impressive technique in
merging the medical images of different modalities. Certainly, the fused
image assists the physician in disease diagnosis for effective treatment
planning. The fusion process combines multi-modal images to incur a
single image with excellent quality, retaining the information of
original images. This paper proposes a multi-modal medical image fusion
through a weighted blending of high-frequency subbands of nonsubsampled
shearlet transform (NSST) domain via chaotic grey wolf optimization
algorithm. As an initial step, the NSST is applied on source images to
decompose into the multi-scale and multi-directional components. The
low-frequency bands are fused based on a simple max rule to sustain the
energy of an individual. The texture details of input images are
preserved by an adaptively weighted combination of high-frequency images
using a recent chaotic grey wolf optimization algorithm to minimize the
distance between the fused image and source images. The entire process
emphasizes on retaining the energy of the low-frequency band and the
transferring of texture features from source images to the fused image.
Finally, the fused image is formed using inverse NSST of merged low and
high-frequency bands. The experiments are carried out on eight different
disease datasets obtained from Brain Atlas, which consists of MR-T1 and
MR-T2, MR and SPECT, MR and PET, and MR and CT. The effectiveness of the
proposed method is validated using more than 100 pairs of images based
on the subjective and objective quality assessment. The experimental
results confirm that the proposed method performs better in contrast
with the current state-of-the-art image fusion techniques in terms of
entropy, VIFF, and FMI. Hence, the proposed method will be helpful for
disease diagnosis, medical treatment planning, and surgical procedure.},
author = {Asha, C. S. and Lal, Shyam and Gurupur, Varadraj Prabhu and Saxena, P. U.Prakash},
doi = {10.1109/ACCESS.2019.2908076},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/05-2-Multi-Modal Bands Using Chaotic Grey Wolf .pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {MRI,NSST,PET,SPECT,chaotic function,grey Wolf optimization,image fusion},
pages = {40782--40796},
publisher = {IEEE},
title = {{Multi-Modal Medical Image Fusion with Adaptive Weighted Combination of NSST Bands Using Chaotic Grey Wolf Optimization}},
volume = {7},
year = {2019}
}
@article{Zhang2013,
abstract = {Recently, sparse representation (SR) and joint sparse representation (JSR) have attracted a lot of interest in image fusion. The SR models signals by sparse linear combinations of prototype signal atoms that make a dictionary. The JSR indicates that different signals from the various sensors of the same scene form an ensemble. These signals have a common sparse component and each individual signal owns an innovation sparse component. The JSR offers lower computational complexity compared with SR. First, for JSR-based image fusion, we give a new fusion rule. Then, motivated by the method of optimal directions (MOD), for JSR, we propose a novel dictionary learning method (MODJSR) whose dictionary updating procedure is derived by employing the JSR structure one time with singular value decomposition (SVD). MODJSR has lower complexity than the K-SVD algorithm which is often used in previous JSR-based fusion algorithms. To capture the image details more efficiently, we proposed the generalized JSR in which the signals ensemble depends on two dictionaries. MODJSR is extended to MODGJSR in this case. MODJSR/MODGJSR can simultaneously carry out dictionary learning, denoising, and fusion of noisy source images. Some experiments are given to demonstrate the validity of the MODJSR/MODGJSR for image fusion. {\textcopyright} 2013 Society of Photo-Optical Instrumentation Engineers (SPIE).},
author = {Zhang, Qiheng and Fu, Yuli and Li, Haifeng and Zou, Jian},
doi = {10.1117/1.oe.52.5.057006},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Dictionary learning method for joint.pdf:pdf},
issn = {0091-3286},
journal = {Optical Engineering},
number = {5},
pages = {057006},
title = {{Dictionary learning method for joint sparse representation-based image fusion}},
volume = {52},
year = {2013}
}
@article{Kim2016,
author = {Kim, Minjae and Han, David K and Ko, Hanseok},
doi = {10.1016/j.inffus.2015.03.003},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Joint patch clustering-based dictionary learning for multimodal imagefusion.pdf:pdf},
issn = {1566-2535},
journal = {Information Fusion},
keywords = {multimodal image fusion,sparse representation},
pages = {198--214},
publisher = {Elsevier B.V.},
title = {{Joint patch clustering-based dictionary learning for multimodal image fusion}},
url = {http://dx.doi.org/10.1016/j.inffus.2015.03.003},
volume = {27},
year = {2016}
}
@article{Meher2019,
abstract = {Image fusion has been emerging as an important area of research. It has attracted many applications such as surveillance, photography, medical diagnosis, etc. Image fusion techniques are developed at three levels: pixel, feature and decision. Region based image fusion is one of the methods of feature level. It possesses certain advantages – less sensitive to noise, more robust and avoids misregistration. This paper presents a review of region based fusion approaches. A first hand classification of region based fusion methods is carried out. A comprehensive list of objective fusion evaluation metrics is highlighted to compare the existing methods. A detailed analysis is carried out and results are presented in tabular form. This may attract researchers to further explore the research in this direction.},
author = {Meher, Bikash and Agrawal, Sanjay and Panda, Rutuparna and Abraham, Ajith},
doi = {10.1016/j.inffus.2018.07.010},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/09-w-01-year2019-04-A survey on region based image fusion methods.pdf:pdf},
issn = {15662535},
journal = {Information Fusion},
keywords = {Image fusion,Region based fusion,Segmentation},
number = {December 2017},
pages = {119--132},
publisher = {Elsevier},
title = {{A survey on region based image fusion methods}},
url = {https://doi.org/10.1016/j.inffus.2018.07.010},
volume = {48},
year = {2019}
}
@article{Du2016,
abstract = {Multi-modal medical image fusion is the process of merging multiple images from single or multiple imaging modalities to improve the imaging quality with preserving the specific features. Medical image fusion covers a broad number of hot topic areas, including image processing, computer vision, pattern recognition, machine learning and artificial intelligence. And medical image fusion has been widely used in clinical for physicians to comprehend the lesion by the fusion of different modalities medical images. In this review, methods in the field of medical image fusion are characterized by (1) image decomposition and image reconstruction, (2) image fusion rules, (3) image quality assessments, and (4) experiments on the benchmark dataset. In addition, this review provides a factual listing of scientific challenges faced in the field of multi-modal medical image fusion.},
author = {Du, Jiao and Li, Weisheng and Lu, Ke and Xiao, Bin},
doi = {10.1016/j.neucom.2015.07.160},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/00 An overview of multi-modal medical image fusion.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Image decomposition,Image fusion,Image fusion rules,Image quality assessment,Image reconstruction,Multi-modal},
pages = {3--20},
title = {{An overview of multi-modal medical image fusion}},
volume = {215},
year = {2016}
}
@article{Liu2017,
abstract = {{\textcopyright} 2017 International Society of Information Fusion (ISIF). Medical image fusion technique plays an an increasingly critical role in many clinical applications by deriving the complementary information from medical images with different modalities. In this paper, a medical image fusion method based on convolutional neural networks (CNNs) is proposed. In our method, a siamese convolutional network is adopted to generate a weight map which integrates the pixel activity information from two source images. The fusion process is conducted in a multi-scale manner via image pyramids to be more consistent with human visual perception. In addition, a local similarity based strategy is applied to adaptively adjust the fusion mode for the decomposed coefficients. Experimental results demonstrate that the proposed method can achieve promising results in terms of both visual quality and objective assessment.},
author = {Liu, Yu and Chen, Xun and Cheng, Juan and Peng, Hu},
doi = {10.23919/ICIF.2017.8009769},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/A Medical Image Fusion .pdf:pdf},
isbn = {9780996452700},
journal = {20th International Conference on Information Fusion, Fusion 2017 - Proceedings},
title = {{A medical image fusion method based on convolutional neural networks}},
year = {2017}
}
@article{Daneshvar2010,
abstract = {Image fusion has become a widely used tool for increasing the interpretation quality of images in medical applications. The acquired data might exhibit either good functional characteristic (such as PET) or high spatial resolution (such as MRI). The MRI image shows the brain tissue anatomy and contains no functional information. The PET image indicates the brain function and has a low spatial resolution. Hence, the image fusion task is carried out to enhance the spatial resolution of the functional images by combining them with a high-resolution anatomic image. A perfect fusion process preserves the original functional characteristics and adds spatial characteristics to the image with no spatial distortion. The intensity-hue-saturation (IHS) algorithm and the retina-inspired model (RIM) fusion technique can preserve more spatial feature and more functional information content, respectively. The presented algorithm integrates the advantages of both IHS and RIM fusion methods to improve the functional and spatial information content. Visual and statistical analyses show that the proposed algorithm significantly improves the fusion quality in terms of: entropy, mutual information, discrepancy, and average gradient; compared to fusion methods including, IHS, Brovey, discrete wavelet transform (DWT), {\`{a}}-trous wavelet and RIM. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Daneshvar, Sabalan and Ghassemian, Hassan},
doi = {10.1016/j.inffus.2009.05.003},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/MRI and PET image fusion by combining IHS and retina-inspired models.pdf:pdf},
issn = {15662535},
journal = {Information Fusion},
keywords = {IHS (intensity-hue-saturation),Image fusion,MRI (magnetic resonance imaging),PET (positron emission tomography),Retina-inspired model (RIM)},
number = {2},
pages = {114--123},
publisher = {Elsevier B.V.},
title = {{MRI and PET image fusion by combining IHS and retina-inspired models}},
url = {http://dx.doi.org/10.1016/j.inffus.2009.05.003},
volume = {11},
year = {2010}
}
@article{Amsterdam,
author = {Amsterdam, S J},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/A NEW QUALITY METRIC FOR IMAGE FUSION .pdf:pdf},
isbn = {0780377508},
journal = {Image (Rochester, N.Y.)},
pages = {173--176},
title = {{Gemma Piella' and Heni Heijmans}}
}
@article{Liu2013,
abstract = {Sparse representation (SR) has been widely used in many image processing applications including image fusion. As the contents vary significantly across different images, a highly redundant dictionary is always required in the sparse model, which reduces the algorithm stability and efficiency. This paper proposes a multi-focus image fusion method based on SR with adaptive sparse domain selection (SR-ASDS). Under SR-ASDS, numerous high-quality image patches are first classified into several categories according to their gradient information, and each category is applied into training a compact sub-dictionary. At the fusion process, a corresponding sub-dictionary is adaptively selected for a given pair of source image patches. Moreover, we present a general optimization framework for the merging rule design of the SR based image fusion. Numerous experiments on both clear images and the noisy ones demonstrate that the proposed method outperforms the fusion methods which use a single dictionary, in terms of several popular objective evaluation criteria.},
author = {Liu, Yu and Wang, Zengfu},
doi = {10.1109/ICIG.2013.123},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Multi-focus image fusion based .pdf:pdf},
isbn = {9780769550503},
journal = {Proceedings - 2013 7th International Conference on Image and Graphics, ICIG 2013},
pages = {591--596},
publisher = {IEEE},
title = {{Multi-focus image fusion based on sparse representation with adaptive sparse domain selection}},
year = {2013}
}
@misc{Manafy2006,
abstract = {The article focuses on literacy and access to books in a time of the proliferation of the so-lled digital natives. A National Literacy Trust research revealed that 86{\%} of young British people own a mobile phone while only 73{\%} of which have books of their own and added that there is a strong link between the reading ability of the youth with their access to books. ReadWriteWeb said that parental supervision is also responsible for the literacy of their young and that there are ways of taking advantage of the advanced technology for the kids to learn.},
author = {Manafy, Michelle},
booktitle = {EContent},
doi = {10.2307/1348337},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/read{\_}me.txt:txt},
issn = {15252531},
number = {4},
pages = {6},
title = {{Read me}},
volume = {29},
year = {2006}
}
@article{Zhang2018,
abstract = {As a result of several successful applications in computer vision and image processing, sparse representation (SR) has attracted significant attention in multi-sensor image fusion. Unlike the traditional multiscale transforms (MSTs) that presume the basis functions, SR learns an over-complete dictionary from a set of training images for image fusion, and it achieves more stable and meaningful representations of the source images. By doing so, the SR-based fusion methods generally outperform the traditional MST image fusion methods in both subjective and objective tests. In addition, they are less susceptible to mis-registration among the source images, thus facilitating the practical applications. This survey paper proposes a systematic review of the SR-based multi-sensor image fusion literature, highlighting the pros and cons of each category of approaches. Specifically, we start by performing a theoretical investigation of the entire system from three key algorithmic aspects, (1) sparse representation models; (2) dictionary learning methods; and (3) activity levels and fusion rules. Subsequently, we show how the existing works address these scientific problems and design the appropriate fusion rules for each application such as multi-focus image fusion and multi-modality (e.g., infrared and visible) image fusion. At last, we carry out some experiments to evaluate the impact of these three algorithmic components on the fusion performance when dealing with different applications. This article is expected to serve as a tutorial and source of reference for researchers preparing to enter the field or who desire to employ the sparse representation theory in other fields.},
author = {Zhang, Qiang and Liu, Yi and Blum, Rick S. and Han, Jungong and Tao, Dacheng},
doi = {10.1016/j.inffus.2017.05.006},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Sparserepresentationbasedmulti-sensorimagefusion.pdf:pdf},
issn = {15662535},
journal = {Information Fusion},
keywords = {Activity level,Dictionary learning,Image fusion,Sparse representation},
pages = {57--75},
publisher = {Elsevier B.V.},
title = {{Sparse representation based multi-sensor image fusion for multi-focus and multi-modality images: A review}},
url = {http://dx.doi.org/10.1016/j.inffus.2017.05.006},
volume = {40},
year = {2018}
}
@article{Wright2009,
author = {Wright, John and Ganesh, Arvind and Yang, Allen and Zhou, Zihan and Ma, Yi},
doi = {10.1109/TPAMI.2008.79},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Robust Face Recognitionvia Sparse Representation.pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {2},
pages = {210 -- 227},
title = {{Sparsity and Robustness in Face Recognition A tutorial on how to apply the models and tools correctly Linear Models for Face Recognition with Varying Illumi- nation}},
volume = {31},
year = {2009}
}
@article{Yu2011a,
abstract = {In this paper, a novel joint sparse representation-based image fusion method is proposed. Since the sensors observe related phenomena, the source images are expected to possess common and innovation features. We use sparse coefficients as image features. The source image is represented with the common and innovation sparse coefficients by joint sparse representation. The sparse coefficients are consequently weighted by the mean absolute values of the innovation coefficients. Furthermore, since sparse representation has been significantly successful in the development of image denoising algorithms, our method can carry out image denoising and fusion simultaneously, while the images are corrupted by additive noise. Experiment results show that the performance of the proposed method is better than that of other methods in terms of several metrics, as well as in the visual quality. {\textcopyright} 2011 IEEE.},
author = {Yu, Nannan and Qiu, Tianshuang and Bi, Feng and Wang, Aiqi},
doi = {10.1109/JSTSP.2011.2112332},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Image Features Extraction and Fusion Based on Joint Sparse Representation(2).pdf:pdf},
issn = {19324553},
journal = {IEEE Journal on Selected Topics in Signal Processing},
keywords = {Features extraction,K-SVD,image fusion,joint sparse representation},
number = {5},
pages = {1074--1082},
publisher = {IEEE},
title = {{Image features extraction and fusion based on joint sparse representation}},
volume = {5},
year = {2011}
}
@article{Li2013,
abstract = {A fast and effective image fusion method is proposed for creating a highly informative fused image through merging multiple images. The proposed method is based on a two-scale decomposition of an image into a base layer containing large scale variations in intensity, and a detail layer capturing small scale details. A novel guided filtering-based weighted average technique is proposed to make full use of spatial consistency for fusion of the base and detail layers. Experimental results demonstrate that the proposed method can obtain state-of-the-art performance for fusion of multispectral, multifocus, multimodal, and multiexposure images.},
author = {Li, Shutao and Kang, Xudong and Hu, Jianwen},
doi = {10.1109/TIP.2013.2244222},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/GF.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Guided filter,image fusion,spatial consistency,two-scale decomposition},
number = {7},
pages = {2864--2875},
title = {{Image fusion with guided filtering}},
volume = {22},
year = {2013}
}
@article{Yin2019a,
author = {Yin, Ming and Liu, Xiaoning and Liu, Yu and Chen, Xun},
doi = {10.1109/TIM.2018.2838778},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/03-10-Medical Image Fusion in NonsubsampledShearlet.pdf:pdf},
issn = {00189456},
journal = {IEEE Transactions on Instrumentation and Measurement},
keywords = {Activity level measure,image fusion,medical imaging,nonsubsampled shearlet transform (NSST),pulse coupled neural network (PCNN)},
number = {1},
pages = {49--64},
publisher = {IEEE},
title = {{Medical Image Fusion with Parameter-Adaptive Pulse Coupled Neural Network in Nonsubsampled Shearlet Transform Domain}},
volume = {68},
year = {2019}
}
@article{Aharon2006,
abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data},
author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
doi = {10.1109/TSP.2006.881199},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/K-SVD An Algorithm for Designing OvercompleteDictionaries for Sparse Representation.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Atom decomposition,Basis pursuit,Codebook,Dictionary,FOCUSS,Gain-shape VQ,K-SVD,K-means,Matching pursuit,Sparse representation,Training,Vector quantization},
number = {11},
pages = {4311--4322},
title = {{K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation}},
volume = {54},
year = {2006}
}
@article{Liu2015,
abstract = {In this study, a novel adaptive sparse representation (ASR) model is presented for simultaneous image fusion and denoising. As a powerful signal modelling technique, sparse representation (SR) has been successfully employed in many image processing applications such as denoising and fusion. In traditional SR-based applications, a highly redundant dictionary is always needed to satisfy signal reconstruction requirement since the structures vary significantly across different image patches. However, it may result in potential visual artefacts as well as high computational cost. In the proposed ASR model, instead of learning a single redundant dictionary, a set of more compact sub-dictionaries are learned from numerous high-quality image patches which have been pre-classified into several corresponding categories based on their gradient information. At the fusion and denoising processes, one of the sub-dictionaries is adaptively selected for a given set of source image patches. Experimental results on multi-focus and multi-modal image sets demonstrate that the ASR-based fusion method can outperform the conventional SR-based method in terms of both visual quality and objective assessment.},
author = {Liu, Yu and Wang, Zengfu},
doi = {10.1049/iet-ipr.2014.0311},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Simultaneous image fusion .pdf:pdf},
issn = {17519659},
journal = {IET Image Processing},
number = {5},
pages = {347--357},
title = {{Simultaneous image fusion and denoising with adaptive sparse representation}},
volume = {9},
year = {2015}
}
@article{Aharon2006a,
abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data},
author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
doi = {10.1109/TSP.2006.881199},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/K-SVD An Algorithm for Designing OvercompleteDictionaries for Sparse Representation.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Atom decomposition,Basis pursuit,Codebook,Dictionary,FOCUSS,Gain-shape VQ,K-SVD,K-means,Matching pursuit,Sparse representation,Training,Vector quantization},
number = {11},
pages = {4311--4322},
title = {{K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation}},
volume = {54},
year = {2006}
}
@article{Hsu2019,
abstract = {{\textcopyright} 2019, CARS. Rationale and objectives: The ultrasound B-mode-based morphological and texture analysis and Nakagami parametric imaging have been proposed to characterize breast tumors. Since these three feature categories of ultrasonic tissue characterization supply information on different physical characteristics of breast tumors, by combining the above methods is expected to provide more clues for classifying breast tumors. Materials and methods: To verify the validity of the concept, raw data were obtained from 160 clinical cases. Six different types of morphological-feature parameters, four texture features, and the Nakagami parameter of benignancy and malignancy were extracted for evaluation. The Pearson's correlation matrix was used to calculate the correlation between different feature parameters. The fuzzy c-means clustering and stepwise regression techniques were utilized to determine the optimal feature set, respectively. The logistic regression, receiver operating characteristic curve, and support vector machine were used to estimate the diagnostic ability. Results: The best performance was obtained by combining morphological-feature parameter (e.g., standard deviation of the shortest distance), texture feature (e.g., variance), and the Nakagami parameter, with an accuracy of 89.4{\%}, a specificity of 86.3{\%}, a sensitivity of 92.5{\%}, and an area under receiver operating characteristic curve of 0.96. There was no significant difference between using fuzzy c-means clustering, logistic regression, and support vector machine based on the optimal feature set for breast tumors classification. Conclusion: Therefore, we verified that different physical ultrasonic features are functionally complementary and thus improve the performance in diagnosing breast tumors. Moreover, the optimal feature set had the maximum discriminating performance should be irrelative to the power of classifiers.},
author = {Hsu, Soa Min and Kuo, Wen Hung and Kuo, Fang Chuan and Liao, Yin Yin},
doi = {10.1007/s11548-018-01908-8},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Hsu et al. - 2019 - Breast tumor classification using different features of quantitative ultrasound parametric images.pdf:pdf},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {Breast ultrasound,Classification,Morphological features,Nakagami parameter,Texture features},
number = {4},
pages = {623--633},
publisher = {Springer International Publishing},
title = {{Breast tumor classification using different features of quantitative ultrasound parametric images}},
url = {https://doi.org/10.1007/s11548-018-01908-8},
volume = {14},
year = {2019}
}
@article{Zhang2018a,
abstract = {As a result of several successful applications in computer vision and image processing, sparse representation (SR) has attracted significant attention in multi-sensor image fusion. Unlike the traditional multiscale transforms (MSTs) that presume the basis functions, SR learns an over-complete dictionary from a set of training images for image fusion, and it achieves more stable and meaningful representations of the source images. By doing so, the SR-based fusion methods generally outperform the traditional MST image fusion methods in both subjective and objective tests. In addition, they are less susceptible to mis-registration among the source images, thus facilitating the practical applications. This survey paper proposes a systematic review of the SR-based multi-sensor image fusion literature, highlighting the pros and cons of each category of approaches. Specifically, we start by performing a theoretical investigation of the entire system from three key algorithmic aspects, (1) sparse representation models; (2) dictionary learning methods; and (3) activity levels and fusion rules. Subsequently, we show how the existing works address these scientific problems and design the appropriate fusion rules for each application such as multi-focus image fusion and multi-modality (e.g., infrared and visible) image fusion. At last, we carry out some experiments to evaluate the impact of these three algorithmic components on the fusion performance when dealing with different applications. This article is expected to serve as a tutorial and source of reference for researchers preparing to enter the field or who desire to employ the sparse representation theory in other fields.},
author = {Zhang, Qiang and Liu, Yi and Blum, Rick S. and Han, Jungong and Tao, Dacheng},
doi = {10.1016/j.inffus.2017.05.006},
file = {:C$\backslash$:/E-DATA-GROUNP/github/baidu{\_}yun/Mendeley{\_}paper/Sparserepresentationbasedmulti-sensorimagefusion.pdf:pdf},
issn = {15662535},
journal = {Information Fusion},
keywords = {Activity level,Dictionary learning,Image fusion,Sparse representation},
pages = {57--75},
publisher = {Elsevier B.V.},
title = {{Sparse representation based multi-sensor image fusion for multi-focus and multi-modality images: A review}},
url = {http://dx.doi.org/10.1016/j.inffus.2017.05.006},
volume = {40},
year = {2018}
}
